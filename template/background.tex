\section{Background}
\label{sec:background}

GP borrows several major ideas from Darwinian evolution that influence the accuracy of symbolic regression. However, GP also inherits one of the major issues with evolution: diversity. Symbolic regression also has its own major issue of overfitting that must be considered when applying a genetic algorithm to symbolic regression. The following sections will discuss the ideas borrowed from evolution that make GP function and the issues that must be accounted for when writing a genetic algorithm.\\

\textbf{Diversity}\\
For GP to function optimally, the population of individuals must be diverse. Without a diverse population, crossover becomes less effective because recombinant individuals are very similar to their parents. Thus, the population begins to stagnate and a reasonable solution can never be reached. Maintaining diversity is extremely important to GP and is a key consideration when setting initial parameters of the genetic algorithm.\\

\textbf{Initial Population}\\
	Diversity begins with the initial population. Both the size of the initial population and the method for generating an individual are extremely important to maintaining diversity. The size of the initial population must be large enough to be adequately dispersed across the problem space. Standard practice in the field of GP is to have a population size of at least 500, although a larger population is always better \cite{poli08:fieldguide}. We used a population of 1000 in our experiments because of time constraints.\\

\textbf{Reproduction}\\
	Reproduction in symbolic regression passes an exact copy of a tree from one generation to the next. Failing to reproduce the most fit individual from one generation to the next could result in the loss of important functions from the population. We reproduced the top 10\% of our population for the next generation. This kept the best genes in the previous population available for the current population, preventing loss of fitness between generations.\\

\textbf{Crossover}\\
	Crossover is the driving force behind improvement in GP. Subtrees from two individuals that are deemed fit are combined together to generate a new individual. When crossing over two individuals, we randomly selected subtrees from each individual and swapped them between the two individuals.  We chose to randomly select crossover points in each individual to promote unique tree re-combinations. This is especially helpful for maintaining diversity when two of the same individuals are crossed multiple times because the likelihood of generating the same child twice is extremely low.\\
	Selection of individuals for crossing over plays a large role in maintaining diversity. We implemented a very popular method known as tournament selection. This selection scheme randomly selects a subset of the current population and from that subset, picks the best individual to be a member of the next generation \cite{Gupta_anoverview}. We specifically chose this method of selection to prevent populations becoming composed of crossovers from a few fit individuals.\\

\textbf{Mutation}\\
	Mutation aids in maintaining diversity by introducing genetic motifs into the population. Often, initial populations will not have all the parts necessary to arrive at an optimal solution, or a generation will lose a necessary part through random selection. Mutation offers a way for populations to recover lost or missing elements of optimal solutions. We used a point-mutation scheme, which changed operator nodes to different operations or altered the value of a terminal node. Point mutations help maintain diversity without introducing problems like overly complex tree structures which can lead to over-fitting of the data.\\

\textbf{Overfitting}\\
When implementing a machine learning algorithm such as GP, the experimenter must be wary of overfitting the training data. A model is said to overfit a dataset if it is specific only to those data. This results in a model that performs very well on the given set of training data, but does not generalize to data outside the training set. In the case of symbolic regression, any set of (x, y) pairs can be fit exactly by an arbitrarily complex polynomial. This does not mean, however, that you have found the actual function generating these points, and will give large errors when exposed to (x, y) pairs generated by that same function, but not included in the training set. To avoid this, we took several preventative measures. \\
Firstly, we implemented the most common technique to avoid overfitting, and split our data into a training set and a test set. We randomly selected 80\% of the total data to be our training set, leaving 20\% to be the test set. This way, when we ran ten iterations of GP, we had data that each best tree had not seen before. We determined the overall winner by evaluating each best tree on the test set.\\
Secondly, we implemented a dynamic depth-limiting strategy to prevent
overly complex hypotheses \cite{Overfitting}.  Since the underlying function was human-generated, we decided it was exceedingly unlikely that it would be an extremely complex expression. Thus we limited the depth of trees in the population, weeding out overly complex hypotheses. This provided equations that were, on the whole, more generalizable and had similar test error and training error. In addition to limiting the depth, we also reduced the number of operators. This is discussed at length in the Experiment section, and greatly simplified trees.\\